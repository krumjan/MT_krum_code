{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Import of required libraries\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from typing import Generator, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM,\n",
    "    Dense,\n",
    "    Input,\n",
    "    Reshape,\n",
    "    Concatenate,\n",
    "    Flatten,\n",
    "    Conv1D,\n",
    ")\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Import of training data, validation data and scalers\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time-variant input\n",
    "train_in_var = np.load(\n",
    "    \"/cluster/home/krum/store/train_in32_var.npy\",\n",
    "    allow_pickle=True,\n",
    ")\n",
    "print(f\"Training input time variant shape: {train_in_var.shape}\")\n",
    "\n",
    "# time-invariant input\n",
    "train_in_con = np.load(\n",
    "    \"/cluster/home/krum/store/train_in32_con.npy\",\n",
    "    allow_pickle=True,\n",
    ")\n",
    "print(f\"Training input time invariant shape: {train_in_con.shape}\")\n",
    "\n",
    "# output\n",
    "train_out = np.load(\n",
    "    \"/cluster/home/krum/store/train_out32.npy\",\n",
    "    allow_pickle=True,\n",
    ")\n",
    "print(f\"Training output shape: {train_out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduction to ~1/4 of original size\n",
    "np.random.seed(42)\n",
    "random_indices = np.random.choice(train_in_var.shape[0], 8000000, replace=False)\n",
    "\n",
    "train_in_var = train_in_var[random_indices, :, :]\n",
    "train_in_con = train_in_con[random_indices, :, :]\n",
    "train_out = train_out[random_indices, :, :]\n",
    "\n",
    "print(train_in_var.shape)\n",
    "print(train_in_con.shape)\n",
    "print(train_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time-variant input\n",
    "val_in_var = np.load(\n",
    "    \"/cluster/home/krum/store/val_in32_var.npy\",\n",
    "    allow_pickle=True,\n",
    ")\n",
    "print(f\"Validation input time variant shape: {val_in_var.shape}\")\n",
    "\n",
    "# time-invariant input\n",
    "val_in_con = np.load(\n",
    "    \"/cluster/home/krum/store/val_in32_con.npy\",\n",
    "    allow_pickle=True,\n",
    ")\n",
    "print(f\"Validation input time invariant shape: {val_in_con.shape}\")\n",
    "\n",
    "# output\n",
    "val_out = np.load(\n",
    "    \"/cluster/home/krum/store/val_out32.npy\",\n",
    "    allow_pickle=True,\n",
    ")\n",
    "print(f\"Validation output shape: {val_out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduction to ~1/4 of original size\n",
    "np.random.seed(42)\n",
    "random_indices = np.random.choice(val_in_var.shape[0], 2600000, replace=False)\n",
    "\n",
    "val_in_var = val_in_var[random_indices, :, :]\n",
    "val_in_con = val_in_con[random_indices, :, :]\n",
    "val_out = val_out[random_indices, :, :]\n",
    "\n",
    "print(val_in_var.shape)\n",
    "print(val_in_con.shape)\n",
    "print(val_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input scaler\n",
    "with open(\"/cluster/home/krum/store/scaler_in.pkl\", \"rb\") as file:\n",
    "    scaler_in = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output scaler\n",
    "with open(\"/cluster/home/krum/store/scaler_out.pkl\", \"rb\") as file:\n",
    "    scaler_out = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Batch generators\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator_train(\n",
    "    batch_size: int,\n",
    ") -> Generator[Tuple[Tuple[Any, Any], Any], None, None]:\n",
    "    \"\"\"\n",
    "    Generates batches of input (numerical and categorical) and output data for training.\n",
    "    The generator shuffles the dataset at the start of each epoch, ensuring every sample\n",
    "    is used exactly once per epoch but in a randomized order.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_size : int\n",
    "        The number of samples per batch\n",
    "    train_data_size : int\n",
    "        The total number of samples in the training dataset\n",
    "\n",
    "    Yields\n",
    "    -------\n",
    "    Tuple[Tuple[Any, Any], Any]\n",
    "        A tuple containing the inputs (numerical and categorical) and the output data for the batch.\n",
    "    \"\"\"\n",
    "    train_data_size = train_in_var.shape[0]\n",
    "    indices = np.arange(train_data_size)\n",
    "    while True:\n",
    "        np.random.shuffle(indices)  # Shuffle indices at the start of each epoch\n",
    "        for start_idx in range(0, train_data_size, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, train_data_size)\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "\n",
    "            input_var, input_con = (\n",
    "                train_in_var[batch_indices],\n",
    "                train_in_con[batch_indices],\n",
    "            )\n",
    "            output = train_out[batch_indices]\n",
    "\n",
    "            yield (input_var, input_con), output\n",
    "\n",
    "\n",
    "def batch_generator_val(\n",
    "    batch_size: int,\n",
    ") -> Generator[Tuple[Tuple[Any, Any], Any], None, None]:\n",
    "    \"\"\"\n",
    "    Generates batches of input (numerical and categorical) and output data for validation.\n",
    "    The generator shuffles the dataset at the start of each epoch, ensuring every sample\n",
    "    is used exactly once per epoch but in a randomized order.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_size : int\n",
    "        The number of samples per batch\n",
    "    val_data_size : int\n",
    "        The total number of samples in the validation dataset\n",
    "\n",
    "    Yields\n",
    "    -------\n",
    "    Tuple[Tuple[Any, Any], Any]\n",
    "        A tuple containing the inputs (numerical and categorical) and the output data for the batch.\n",
    "    \"\"\"\n",
    "    val_data_size = val_in_var.shape[0]\n",
    "    indices = np.arange(val_data_size)\n",
    "    while True:\n",
    "        np.random.shuffle(indices)  # Shuffle indices at the start of each epoch\n",
    "        for start_idx in range(0, val_data_size, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, val_data_size)\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "\n",
    "            input_var, input_con = (\n",
    "                val_in_var[batch_indices],\n",
    "                val_in_con[batch_indices],\n",
    "            )\n",
    "            output = val_out[batch_indices]\n",
    "\n",
    "            yield (input_var, input_con), output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Custom measures, callbacks and loss function\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting min and max values from scaler\n",
    "lat_min = scaler_out.data_min_[0]\n",
    "lat_max = scaler_out.data_max_[0]\n",
    "lon_min = scaler_out.data_min_[1]\n",
    "lon_max = scaler_out.data_max_[1]\n",
    "alt_min = scaler_out.data_min_[2]\n",
    "alt_max = scaler_out.data_max_[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_lat(y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates the RMSE between the true and predicted latitude values after\n",
    "    adjusting for Min-Max scaling.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : tf.Tensor\n",
    "        The true latitude values\n",
    "    y_pred : tf.Tensor\n",
    "        The predicted latitude values\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    tf.Tensor\n",
    "        RMSE value between the true and predicted latitude values\n",
    "    \"\"\"\n",
    "    lat_true = tf.gather(y_true, [0], axis=2)\n",
    "    lat_true = tf.cast(lat_true, tf.float32)\n",
    "    lat_pred = tf.gather(y_pred, [0], axis=2)\n",
    "    lat_pred = tf.cast(lat_pred, tf.float32)\n",
    "\n",
    "    # Adjusting for Min-Max scaling\n",
    "    lat_min_f = tf.cast((tf.fill(tf.shape(lat_true), lat_min)), tf.float32)\n",
    "    lat_max_f = tf.cast((tf.fill(tf.shape(lat_pred), lat_max)), tf.float32)\n",
    "\n",
    "    # Reverse the Min-Max scaling\n",
    "    lat_true_unnorm = lat_true * (lat_max_f - lat_min_f) + lat_min_f\n",
    "    lat_pred_unnorm = lat_pred * (lat_max_f - lat_min_f) + lat_min_f\n",
    "\n",
    "    return tf.math.sqrt(\n",
    "        tf.math.reduce_mean(tf.math.square(lat_pred_unnorm - lat_true_unnorm))\n",
    "    )\n",
    "\n",
    "\n",
    "def rmse_lon(y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates the RMSE between the true and predicted longitude values after\n",
    "    adjusting for Min-Max scaling.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : tf.Tensor\n",
    "        The true longitude values\n",
    "    y_pred : tf.Tensor\n",
    "        The predicted longitude values\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    tf.Tensor\n",
    "        RMSE value between the true and predicted longitude values\n",
    "    \"\"\"\n",
    "    lon_true = tf.gather(y_true, [1], axis=2)\n",
    "    lon_true = tf.cast(lon_true, tf.float32)\n",
    "    lon_pred = tf.gather(y_pred, [1], axis=2)\n",
    "    lon_pred = tf.cast(lon_pred, tf.float32)\n",
    "\n",
    "    # Adjusting for Min-Max scaling\n",
    "    lon_min_f = tf.cast((tf.fill(tf.shape(lon_true), lon_min)), tf.float32)\n",
    "    lon_max_f = tf.cast((tf.fill(tf.shape(lon_pred), lon_max)), tf.float32)\n",
    "\n",
    "    # Reverse the Min-Max scaling\n",
    "    lon_true_unnorm = lon_true * (lon_max_f - lon_min_f) + lon_min_f\n",
    "    lon_pred_unnorm = lon_pred * (lon_max_f - lon_min_f) + lon_min_f\n",
    "\n",
    "    return tf.math.sqrt(\n",
    "        tf.math.reduce_mean(tf.math.square(lon_pred_unnorm - lon_true_unnorm))\n",
    "    )\n",
    "\n",
    "\n",
    "def rmse_alt(y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates the RMSE between the true and predicted altitude values after\n",
    "    adjusting for Min-Max scaling.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : tf.Tensor\n",
    "        The true altitude values\n",
    "    y_pred : tf.Tensor\n",
    "        The predicted altitude values\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    tf.Tensor\n",
    "        RMSE value between the true and predicted altitude values\n",
    "    \"\"\"\n",
    "    alt_true = tf.gather(y_true, [2], axis=2)\n",
    "    alt_true = tf.cast(alt_true, tf.float32)\n",
    "    alt_pred = tf.gather(y_pred, [2], axis=2)\n",
    "    alt_pred = tf.cast(alt_pred, tf.float32)\n",
    "\n",
    "    # Adjusting for Min-Max scaling\n",
    "    alt_min_f = tf.cast((tf.fill(tf.shape(alt_true), alt_min)), tf.float32)\n",
    "    alt_max_f = tf.cast((tf.fill(tf.shape(alt_pred), alt_max)), tf.float32)\n",
    "\n",
    "    # Reverse the Min-Max scaling\n",
    "    alt_true_unnorm = alt_true * (alt_max_f - alt_min_f) + alt_min_f\n",
    "    alt_pred_unnorm = alt_pred * (alt_max_f - alt_min_f) + alt_min_f\n",
    "\n",
    "    return tf.math.sqrt(\n",
    "        tf.math.reduce_mean(tf.math.square(alt_pred_unnorm - alt_true_unnorm))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback to reduce learning rate on plateau\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.8,\n",
    "    patience=6,\n",
    "    min_delta=0.0001,\n",
    "    mode=\"min\",\n",
    "    min_lr=0.0001,\n",
    ")\n",
    "\n",
    "# Callback for early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=20,\n",
    "    min_delta=0.0001,\n",
    "    mode=\"min\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function (Weighted MSE)\n",
    "def custom_mse(weights):\n",
    "    def weighted_mse(gt, pred):\n",
    "        return K.sum(weights * K.square(gt - pred)) / K.sum(weights)\n",
    "\n",
    "    return weighted_mse\n",
    "\n",
    "\n",
    "# Defining the weights\n",
    "weights = np.linspace(1, 0.1, 37)\n",
    "weights = np.tile(weights, (3, 1)).T.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Sweep\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Size of model inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define shape of time dependent input\n",
    "t_input_sequence_length = train_in_var.shape[1]\n",
    "t_input_features = train_in_var.shape[2]\n",
    "print(\n",
    "    f\"Time variant input: {t_input_sequence_length} timesteps with {t_input_features} features\"\n",
    ")\n",
    "\n",
    "# Define shape of constant input\n",
    "c_input_sequence_length = train_in_con.shape[1]\n",
    "c_input_features = train_in_con.shape[2]\n",
    "print(\n",
    "    f\"Time constant input: {c_input_sequence_length} timesteps with {c_input_features} features\"\n",
    ")\n",
    "\n",
    "# Define shape of output\n",
    "output_sequence_length = train_out.shape[1]\n",
    "output_features = train_out.shape[2]\n",
    "print(f\"Output: {output_sequence_length} timesteps with {output_features} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function to build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(config):\n",
    "\n",
    "    # Define two sets of inputs\n",
    "    input_t = Input(shape=(t_input_sequence_length, t_input_features))\n",
    "    input_c = Input(shape=(c_input_sequence_length, c_input_features))\n",
    "\n",
    "    # First branch (Temporal)\n",
    "    conv_outputs = []\n",
    "    for i in range(t_input_features):\n",
    "        x = Conv1D(\n",
    "            filters=config.conv_filters,\n",
    "            kernel_size=config.kernel_size,\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "        )(input_t[..., i : i + 1])\n",
    "        for i in range(config.conv_layers - 1):\n",
    "            x = Conv1D(\n",
    "                filters=config.conv_filters,\n",
    "                kernel_size=config.kernel_size,\n",
    "                padding=\"same\",\n",
    "                activation=\"relu\",\n",
    "            )(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(32, activation=\"relu\")(x)\n",
    "        conv_outputs.append(x)\n",
    "\n",
    "    t = Concatenate()(conv_outputs)\n",
    "\n",
    "    # Second branch (Constant features)\n",
    "    c = Flatten()(input_c)\n",
    "    c = Dense(32, activation=\"relu\")(c)\n",
    "    c = Dense(32, activation=\"relu\")(c)\n",
    "    c = Dense(32, activation=\"relu\")(c)\n",
    "\n",
    "    # Combine the outputs of the two branches\n",
    "    combined = Concatenate()([t, c])\n",
    "\n",
    "    # Apply dense layers after combining\n",
    "    z = Reshape((9, 32))(combined)\n",
    "    for _ in range(config.num_lstm_layers - 1):\n",
    "        z = LSTM(config.lstm_units, return_sequences=True)(z)\n",
    "    z = LSTM(config.lstm_units, return_sequences=False)(z)\n",
    "    z = Dense(output_sequence_length * output_features, activation=\"linear\")(z)\n",
    "    z = Reshape((output_sequence_length, output_features))(z)\n",
    "\n",
    "    # Build model\n",
    "    model = Model(inputs=[input_t, input_c], outputs=z)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sweep configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"method\": \"random\",  # Random search\n",
    "    \"name\": \"architecture-sweep\",\n",
    "    \"metric\": {\"name\": \"epoch/val_loss\", \"goal\": \"minimize\"},\n",
    "    \"parameters\": {\n",
    "        \"conv_filters\": {\"values\": [16, 32, 64]},\n",
    "        \"conv_layers\": {\"values\": [2, 3, 4]},\n",
    "        \"kernel_size\": {\"values\": [3, 5]},\n",
    "        \"lstm_units\": {\"values\": [16, 32, 64]},\n",
    "        \"num_lstm_layers\": {\"values\": [3, 4, 2]},\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sweep execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "\n",
    "    # Initialize a new wandb run\n",
    "    wandb.init()\n",
    "    config = wandb.config\n",
    "\n",
    "    # Build the model\n",
    "    model = build_model(config)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        loss=custom_mse(weights),\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        metrics=[rmse_lat, rmse_lon, rmse_alt],\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        batch_generator_train(1024),\n",
    "        epochs=100,\n",
    "        steps_per_epoch=7813,\n",
    "        validation_steps=2540,\n",
    "        validation_data=batch_generator_val(1024),\n",
    "        shuffle=False,\n",
    "        callbacks=[\n",
    "            WandbMetricsLogger(200),\n",
    "            reduce_lr,\n",
    "            early_stopping,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "# Initialize the sweep\n",
    "sweep_id = wandb.sweep(\n",
    "    sweep=sweep_config, entity=\"zhaw_zav\", project=\"MIAR_4Departures\"\n",
    ")\n",
    "# sweep_id = 'zhaw_zav/MIAR_4Departures/xggo6krv'\n",
    "\n",
    "# Run the sweep\n",
    "wandb.agent(sweep_id, function=train, count=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
